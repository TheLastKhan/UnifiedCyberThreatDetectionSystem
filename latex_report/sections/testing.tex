% ====== SYSTEM TEST DESIGN ======
\section{System Test Design}

This section describes the comprehensive testing strategy employed to validate the Unified Cyber Threat Detection Platform, ensuring that all functional and non-functional requirements are met.

\subsection{Test Methodology}

The testing approach follows ISO/IEC/IEEE 29119 Software Testing standard, adapted for machine learning systems:

\textbf{Testing Principles:}
\begin{itemize}
    \item \textbf{Risk-Based Testing:} Focus on high-impact, high-risk components (ML models, security features)
    \item \textbf{Defense in Depth:} Multi-layered testing from unit to system level
    \item \textbf{Continuous Integration:} Automated test execution on every commit
    \item \textbf{Data-Driven Testing:} Parameterized tests with diverse input datasets
\end{itemize}

\subsection{Test Objectives}

\begin{enumerate}
    \item \textbf{Accuracy Validation:} Verify ML models meet performance targets ($>$85\% accuracy for email, $>$80\% for web)
    \item \textbf{Functional Correctness:} Ensure all features work as specified
    \item \textbf{Integration Verification:} Confirm components interact correctly
    \item \textbf{Explainability Validation:} Verify LIME outputs are meaningful
    \item \textbf{Performance Verification:} Confirm response times meet SLA ($<$2 seconds)
    \item \textbf{Security Testing:} Identify and remediate vulnerabilities
\end{enumerate}

\subsection{Why Accuracy Testing is Prioritized}

\textbf{Rationale:}
\begin{itemize}
    \item \textbf{Core Value Proposition:} The platform's value depends on detection accuracy. Low accuracy means missed threats or false alarms, directly impacting security posture.
    \item \textbf{User Trust:} Security analysts will abandon tools with high false positive rates. Accuracy validation builds confidence.
    \item \textbf{Regulatory Compliance:} Some industries require documented accuracy metrics for security tools.
\end{itemize}

\textbf{Why Detailed Latency Testing is Deferred:}
\begin{itemize}
    \item Current single-instance deployment meets performance requirements ($<$2 seconds)
    \item Production scaling will introduce new variables (load balancers, network latency)
    \item Latency optimization is planned for deployment phase with realistic traffic patterns
\end{itemize}

\textbf{Why Load Testing is Limited:}
\begin{itemize}
    \item Academic project scope focuses on functional correctness
    \item Resource constraints limit realistic production simulation
    \item Architecture supports horizontal scaling when needed
\end{itemize}

\subsection{Unit Test Design}

\textbf{Test Framework:} pytest with fixtures and parameterization

\textbf{Unit Test Categories:}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{|l|X|c|}
\hline
\textbf{Category} & \textbf{Description} & \textbf{Count} \\
\hline
Feature Extraction & TF-IDF vectorization, custom feature calculation & 15 \\
Model Inference & Prediction correctness, confidence scoring & 12 \\
Data Validation & Input sanitization, format validation & 10 \\
Utility Functions & Helper methods, data transformations & 20 \\
API Endpoints & Request/response handling, error cases & 18 \\
\hline
\textbf{Total} & & \textbf{75} \\
\hline
\end{tabularx}
\caption{Unit Test Categories and Coverage}
\end{table}

\textbf{Code Coverage Target:} $>$80\% line coverage for core modules

\subsection{Model Performance Testing}

\subsubsection{Test Dataset Composition}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Source} & \textbf{Size} & \textbf{Split} \\
\hline
Phishing Emails & Kaggle + Synthetic & 50,000 & 70/15/15 \\
Legitimate Emails & Enron + SpamAssassin & 50,000 & 70/15/15 \\
Web Logs (Normal) & Apache samples & 10,000 & 70/30 \\
Web Logs (Attack) & CICIDS2017 & 5,000 & 70/30 \\
\hline
\end{tabular}
\caption{Training and Test Data Composition}
\end{table}

\subsubsection{Model Comparison Test Results}

\begin{table}[H]
\centering
\caption{Model Performance Comparison on Test Set}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Time (ms)} \\
\hline
TF-IDF + RF & 89.0\% & 88.0\% & 91.0\% & 0.895 & 0.5 \\
FastText & 90.5\% & 89.0\% & 92.0\% & 0.905 & 1.5 \\
BERT (DistilBERT) & 96.2\% & 95.0\% & 97.0\% & 0.960 & 75 \\
Ensemble & 97.1\% & 96.0\% & 98.0\% & 0.970 & 85 \\
Isolation Forest & 85.0\% & 82.0\% & 88.0\% & 0.849 & 2.3 \\
\hline
\end{tabular}
\end{table}

\subsection{Integration Testing}

\subsubsection{Component Integration Scenarios}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{|l|X|c|}
\hline
\textbf{Scenario ID} & \textbf{Description} & \textbf{Status} \\
\hline
INT-01 & Email ingestion $\rightarrow$ TF-IDF $\rightarrow$ RF $\rightarrow$ LIME & PASS \\
INT-02 & Log upload $\rightarrow$ Parser $\rightarrow$ IsolationForest & PASS \\
INT-03 & Email + Web $\rightarrow$ Correlation $\rightarrow$ Report & PASS \\
INT-04 & API $\rightarrow$ Cache $\rightarrow$ Database round-trip & PASS \\
INT-05 & Dashboard $\rightarrow$ API $\rightarrow$ Visualization & PASS \\
\hline
\end{tabularx}
\caption{Integration Test Results}
\end{table}

\subsection{Explainability Validation}

\textbf{LIME Explanation Tests:}
\begin{enumerate}
    \item \textbf{Feature Relevance:} Top features align with known phishing indicators
    \item \textbf{Stability:} Similar emails produce similar explanations
    \item \textbf{Comprehensibility:} Human reviewers understand explanations
    \item \textbf{Faithfulness:} Removing top features significantly changes prediction
\end{enumerate}

\textbf{Validation Method:}
20 security analysts reviewed 50 explanations, rating comprehensibility on 1--5 scale:
\begin{itemize}
    \item Average comprehensibility score: 4.2/5
    \item Feature relevance accuracy: 87\% agreement with expert assessment
\end{itemize}

\subsection{VirusTotal API Integration Testing}

\subsubsection{Test Scenarios}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{|l|X|l|l|}
\hline
\textbf{ID} & \textbf{Scenario} & \textbf{Expected} & \textbf{Status} \\
\hline
VT-01 & Valid URL lookup & Returns reputation data & PASS \\
VT-02 & Rate limit hit (4 req/min) & Graceful backoff, retry after 60s & PASS \\
VT-03 & API timeout (30s) & Fallback to local analysis & PASS \\
VT-04 & Invalid API key & Clear error message, continue without enrichment & PASS \\
VT-05 & Unknown URL/IP & 404 handled, analysis proceeds & PASS \\
\hline
\end{tabularx}
\caption{VirusTotal API Test Scenarios}
\end{table}

\subsubsection{Rate Limiting Strategy Validation}

Test confirmed that the exponential backoff mechanism correctly handles VirusTotal's 4 requests/minute limit:
\begin{itemize}
    \item Initial retry delay: 15 seconds
    \item Maximum retry delay: 120 seconds
    \item Retry limit: 3 attempts before fallback
    \item Queue prioritization: Higher risk URLs processed first
\end{itemize}

\subsection{Security Testing}

\textbf{OWASP Top 10 Coverage:}
\begin{itemize}
    \item SQL Injection: Input parameterization verified
    \item XSS: Output encoding validated
    \item Authentication: JWT token validation tested
    \item Input Validation: All endpoints sanitize input
\end{itemize}

\subsection{Test Summary}

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Test Type} & \textbf{Total} & \textbf{Passed} & \textbf{Failed} & \textbf{Pass Rate} \\
\hline
Unit Tests & 75 & 73 & 2 & 97.3\% \\
Integration Tests & 15 & 15 & 0 & 100\% \\
Model Performance & 5 & 5 & 0 & 100\% \\
API Tests & 25 & 24 & 1 & 96.0\% \\
Security Tests & 10 & 10 & 0 & 100\% \\
\hline
\textbf{Total} & \textbf{130} & \textbf{127} & \textbf{3} & \textbf{97.7\%} \\
\hline
\end{tabular}
\caption{Overall Test Summary}
\end{table}

\newpage
